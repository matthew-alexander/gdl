{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knob_weight = 0.5\n",
    "input = 2\n",
    "goal_prediction = .8\n",
    "\n",
    "step_amount = 0.001\n",
    "\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a form of gradient descent\n",
    "where we update the \"knob_weight\" with the \"direction\" and amount\n",
    "\n",
    "The key to learning is adjusting the weights in the correct *direction* and by the correct *amount* so that the **error** reduces to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Error: 0.03999999999999998 Prediction: 1.0\n",
      "Iteration: 1 Error: 0.3599999999999998 Prediction: 0.20000000000000018\n",
      "Iteration: 2 Error: 3.2399999999999984 Prediction: 2.5999999999999996\n",
      "Iteration: 3 Error: 29.159999999999986 Prediction: -4.599999999999999\n",
      "Iteration: 4 Error: 262.4399999999999 Prediction: 16.999999999999996\n",
      "Iteration: 5 Error: 2361.959999999998 Prediction: -47.79999999999998\n",
      "Iteration: 6 Error: 21257.639999999978 Prediction: 146.59999999999994\n",
      "Iteration: 7 Error: 191318.75999999983 Prediction: -436.5999999999998\n",
      "Iteration: 8 Error: 1721868.839999999 Prediction: 1312.9999999999995\n",
      "Iteration: 9 Error: 15496819.559999991 Prediction: -3935.799999999999\n",
      "Iteration: 10 Error: 139471376.03999993 Prediction: 11810.599999999997\n",
      "Iteration: 11 Error: 1255242384.3599997 Prediction: -35428.59999999999\n",
      "Iteration: 12 Error: 11297181459.239996 Prediction: 106288.99999999999\n",
      "Iteration: 13 Error: 101674633133.15994 Prediction: -318863.79999999993\n",
      "Iteration: 14 Error: 915071698198.4395 Prediction: 956594.5999999997\n",
      "Iteration: 15 Error: 8235645283785.954 Prediction: -2869780.599999999\n",
      "Iteration: 16 Error: 74120807554073.56 Prediction: 8609344.999999996\n",
      "Iteration: 17 Error: 667087267986662.1 Prediction: -25828031.799999986\n",
      "Iteration: 18 Error: 6003785411879960.0 Prediction: 77484098.59999996\n",
      "Iteration: 19 Error: 5.403406870691965e+16 Prediction: -232452292.5999999\n",
      "Iteration: 20 Error: 4.8630661836227686e+17 Prediction: 697356880.9999998\n",
      "Iteration: 21 Error: 4.376759565260492e+18 Prediction: -2092070639.7999995\n",
      "Iteration: 22 Error: 3.939083608734443e+19 Prediction: 6276211922.599998\n",
      "Iteration: 23 Error: 3.545175247860998e+20 Prediction: -18828635764.599995\n",
      "Iteration: 24 Error: 3.1906577230748986e+21 Prediction: 56485907296.999985\n",
      "Iteration: 25 Error: 2.8715919507674074e+22 Prediction: -169457721887.79993\n",
      "Iteration: 26 Error: 2.5844327556906668e+23 Prediction: 508373165666.59973\n",
      "Iteration: 27 Error: 2.3259894801215998e+24 Prediction: -1525119496996.599\n",
      "Iteration: 28 Error: 2.0933905321094406e+25 Prediction: 4575358490992.998\n",
      "Iteration: 29 Error: 1.884051478898497e+26 Prediction: -13726075472975.795\n",
      "Iteration: 30 Error: 1.6956463310086472e+27 Prediction: 41178226418930.586\n",
      "Iteration: 31 Error: 1.5260816979077823e+28 Prediction: -123534679256788.56\n",
      "Iteration: 32 Error: 1.373473528117004e+29 Prediction: 370604037770368.9\n",
      "Iteration: 33 Error: 1.2361261753053034e+30 Prediction: -1111812113311103.4\n",
      "Iteration: 34 Error: 1.112513557774773e+31 Prediction: 3335436339933313.0\n",
      "Iteration: 35 Error: 1.0012622019972956e+32 Prediction: -1.0006309019799936e+16\n",
      "Iteration: 36 Error: 9.01135981797566e+32 Prediction: 3.001892705939981e+16\n",
      "Iteration: 37 Error: 8.110223836178094e+33 Prediction: -9.005678117819942e+16\n",
      "Iteration: 38 Error: 7.299201452560284e+34 Prediction: 2.7017034353459827e+17\n",
      "Iteration: 39 Error: 6.5692813073042565e+35 Prediction: -8.105110306037948e+17\n",
      "Iteration: 40 Error: 5.91235317657383e+36 Prediction: 2.4315330918113843e+18\n",
      "Iteration: 41 Error: 5.321117858916447e+37 Prediction: -7.294599275434153e+18\n",
      "Iteration: 42 Error: 4.7890060730248024e+38 Prediction: 2.188379782630246e+19\n",
      "Iteration: 43 Error: 4.3101054657223227e+39 Prediction: -6.565139347890738e+19\n",
      "Iteration: 44 Error: 3.8790949191500895e+40 Prediction: 1.9695418043672212e+20\n",
      "Iteration: 45 Error: 3.49118542723508e+41 Prediction: -5.908625413101663e+20\n",
      "Iteration: 46 Error: 3.1420668845115716e+42 Prediction: 1.772587623930499e+21\n",
      "Iteration: 47 Error: 2.8278601960604143e+43 Prediction: -5.317762871791496e+21\n",
      "Iteration: 48 Error: 2.5450741764543726e+44 Prediction: 1.5953288615374489e+22\n",
      "Iteration: 49 Error: 2.290566758808935e+45 Prediction: -4.785986584612346e+22\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(50):\n",
    "    \n",
    "    prediction = input * knob_weight\n",
    "    \n",
    "    error = (goal_prediction-prediction) ** 2 \n",
    "    \n",
    "    \n",
    "    derivative = (prediction - goal_prediction) * input\n",
    "\n",
    "    knob_weight = knob_weight - derivative\n",
    "    \n",
    "    if iteration % display_step == 0:\n",
    "        print(\"Iteration: \" + str(iteration) + \" Error: \" + str(error) + \" Prediction: \" + str(prediction))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The key concept: \n",
    "\n",
    "## \"Learning is adjusting our weight to reduce the error to zero\"\n",
    "\n",
    "First, our neural network is really just a bunch of **weights** and\n",
    "an **error** function. Our goal is to move the weights (in this case just one) so that our error goes\n",
    "to zero! How do we do this? Well, it turns out we can take the derivative between two variables\n",
    "in any function. When we do this, we learn how one changes when we change the other. We can\n",
    "then use this to modify each weight in the **direction** that leads us to the lowest error. How do we\n",
    "know this direction, it is always in the opposite of the value of our derivative. (p. 41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Divergence \n",
    "\n",
    "its pretty easy to break this -- just put 2 in as an input. What happens is that small changes to the knob cause huge changes in the prediction and lead to instability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
